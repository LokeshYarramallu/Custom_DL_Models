# Custom_DL_Models

Each layer of a neural network detects the each features of a Neural network

HyperParameter tuning :


| METHOD      |TECHNIQUES| 
|:-------------:|:-------------:|
| **Initialization** | *Xavier (uniform)(Softmax and tanh)*|
|| *He (Possion)(ReLu)*|
| **Activation functions** | *Sigmoid* |
||*Tanh*| 
||*ReLU*| 
||*LeakyReLU*| 
||*Softmax*| 
||*ELU*|
||*Softplus*|  
||*GELU*| 
| **Loss functions** | *Binary Cross Entropy* | 
||*Categorial Cross Entropy*| 
||*Mean Squared Error*|
||*Hinge Loss*| 
||*Smooth Hinge Loss*| 
||*Poisson Loss*|
||*Huber Loss*|   
| **Optimizers and Learning Algo (Back Propagation)** | *Gradient Decent* | 
||*SGD*| 
||*Batch & MiniBatch SGD*| 
||*Momentum*| 
||*Adagrad*| 
||*RMS Prop*| 
||*AdaDelta*| 
||*Adam*| 
| **Regularizations** | *L2* |
||*Dropout Layers*| 
||*Early Stopping*| 
||*Batch Normalization*| 
||*Data Augmentaition*|
||*Ensemble Methods*| 
 
 
