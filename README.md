# Custom_DL_Models

Each layer of a neural network detects the each features of a Neural network

HyperParameter tuning :

Initialization Methods :
Xavier (uniform)(Softmax and tanh)
He (Possion)(ReLu)

Activation-Functions:
Sigmoid
Tanh
RELU
Leaky Relu
Softmax
GELU
ELU
SELU
Softplus

Loss Functions :
Binary Cross Entropy
Categorical Cross Entropy
Mean Squared Error
Hinge Loss
Smooth Hinge Loss
Poisson Loss
Huber Loss

Optimizers and Learning Algo ( Back Propagation ):
Gradient Decent
Stochastic Gradient Decent
Batch & Mini Batch SGD
Momentum
Adagrad
RMSprop
AdaDelta
Adam

Regularizations :
L2 (Weight Decay)
Dropout layers
Early Stopping
Batch Normalization
Dataset augmentation
Ensemble Methods
